import xlrd
import pandas as pd
import numpy as np
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering

# Pulling the file
file_loc = 'documents\your_file.xlsx'
df = pd.read_excel(file_loc, index_col = None, na_values= ['NA'], usecols = "B")
df = df.values.tolist()


#Levenshtein Distance Calculator
def minimumEditDistance(s1, s2):

    if len(s1) > len(s2):
        s1, s2 = s2, s1
    distances = range(len(s1) + 1)
    for index2, char2 in enumerate(s2):
        newDistances = [index2 + 1]
        for index1, char1 in enumerate(s1):
            if char1 == char2:
                newDistances.append(distances[index1])
            else:
                newDistances.append(1 + min((distances[index1], distances[index1 + 1], newDistances[-1])))
        distances = newDistances
    return distances[-1]


levenshteinMatrix = np.zeros((len(df), len(df)), dtype=np.int)

#Populating the Matrix with levenshtein values, only doing a half of the matrix to save time
for i in range(0, len(df)):
    print(i)
    for j in range(0, i):
        char1 = max(df[i],key=len)
        char2 = max(df[j],key=len)
        distance = minimumEditDistance(char1, char2)
        theMatrix[i, j] = distance

levMatrixTrans = np.maximum(dino,dino.transpose())

#duplicating the Matrix across the other half
reducedMatrix = squareform(levMatrixTrans)

#Implementing the Hierarchical Clustering
Z = linkage(reducedMatrix, 'ward')

theNum = 150

dfCategories = AgglomerativeClustering(n_clusters=theNum, affinity='euclidean', linkage='ward')
index = dfCategories.fit_predict(Z)

np.savetxt('B_data.dat', index)


theNum = 300

dfCategories = AgglomerativeClustering(n_clusters=theNum, affinity='euclidean', linkage='ward')
index = dfCategories.fit_predict(Z)

np.savetxt('A_data.dat', index)

fig = plt.figure()
dn = dendrogram(Z, labels = df, orientation = 'left')
plt.show()
